{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.23.3)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision import datasets\n",
    "from matplotlib.pyplot import imshow\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 3689\n",
      "    Root location: ./data/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "\n",
      "Validation data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 495\n",
      "    Root location: ./data/validation\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "\n",
      "Test data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 494\n",
      "    Root location: ./data/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(180, 180)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dir = \"./data/train\"  # path to the train folder\n",
    "validation_dir = \"./data/validation\"  # path to the validation folder\n",
    "test_dir = \"./data/test\"  # path to test folder\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "\n",
    "validation_data = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "print(\n",
    "    f\"Train data:\\n{train_data}\\n\\nValidation data:\\n{validation_data}\\n\\nTest data:\\n{test_data}\"\n",
    ")\n",
    "\n",
    "data_loaders = {\n",
    "    \"train\": DataLoader(train_data, batch_size=4, shuffle=True, num_workers=2),\n",
    "    \"validation\": DataLoader(\n",
    "        validation_data, batch_size=4, shuffle=True, num_workers=2\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modelv3 import SimpleCNN3\n",
    "\n",
    "model = SimpleCNN3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / size\n",
    "    return train_loss / len(dataloader), accuracy\n",
    "\n",
    "\n",
    "def validate(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    validation_loss, correct = 0, 0\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            validation_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / size\n",
    "    return validation_loss / len(dataloader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizers = {\n",
    "    \"SGD\": lambda params: SGD(params, lr=0.001),\n",
    "    \"Adam\": lambda params: Adam(params, lr=0.001),\n",
    "}\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate: 1e-05\n",
      "Epoch 1/3\n",
      " Training Accuracy: 69.31%, Training Loss: 569.4357\n",
      " Validation Accuracy: 69.49%, Validation Loss: 75.0760\n",
      "Train Accuracy: 69.31%, Validation Accuracy: 69.49%\n",
      "Epoch 2/3\n",
      " Training Accuracy: 69.88%, Training Loss: 554.5916\n",
      " Validation Accuracy: 69.49%, Validation Loss: 74.5370\n",
      "Train Accuracy: 69.88%, Validation Accuracy: 69.49%\n",
      "Epoch 3/3\n",
      " Training Accuracy: 69.88%, Training Loss: 546.2582\n",
      " Validation Accuracy: 69.49%, Validation Loss: 74.0367\n",
      "Train Accuracy: 69.88%, Validation Accuracy: 69.49%\n",
      "Last Train Accuracy for LR=1e-05: 69.88%\n",
      "Training with learning rate: 0.0001\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "results = {\"optimizer\": [], \"train_acc\": [], \"validation_acc\": []}\n",
    "\n",
    "for optimizer_name, optimizer_fn in optimizers.items():\n",
    "    print(f\"Training with optimizer: {optimizer_name}\")\n",
    "    model = SimpleCNN().to(device)  # Reinitialize model for each optimizer\n",
    "    optimizer = optimizer_fn(model.parameters())\n",
    "\n",
    "    train_accuracies = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        train_loss, train_acc = train(data_loaders[\"train\"], model, loss_fn, optimizer)\n",
    "        val_loss, val_acc = validate(data_loaders[\"validation\"], model, loss_fn)\n",
    "\n",
    "        train_accuracies.append(train_acc)\n",
    "        validation_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Train Accuracy: {train_acc:.2f}%, Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    results[\"optimizer\"].append(optimizer_name)\n",
    "    results[\"train_acc\"].append(train_accuracies[-1])  # Final train accuracy\n",
    "    results[\"validation_acc\"].append(\n",
    "        validation_accuracies[-1]\n",
    "    )  # Final validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "x = range(len(results[\"optimizer\"]))\n",
    "\n",
    "# Train accuracy\n",
    "plt.bar(x, results[\"train_acc\"], width=0.4, label=\"Train Accuracy\", align=\"center\")\n",
    "# Validation accuracy\n",
    "plt.bar(\n",
    "    [i + 0.4 for i in x],\n",
    "    results[\"validation_acc\"],\n",
    "    width=0.4,\n",
    "    label=\"Validation Accuracy\",\n",
    "    align=\"center\",\n",
    ")\n",
    "\n",
    "plt.xticks([i + 0.2 for i in x], results[\"optimizer\"])\n",
    "plt.title(\"Optimizer Comparison: Train vs Validation Accuracy\")\n",
    "plt.xlabel(\"Optimizer\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
