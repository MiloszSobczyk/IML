{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision import datasets\n",
    "from matplotlib.pyplot import imshow\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 3689\n",
      "    Root location: ../data/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "\n",
      "Validation data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 495\n",
      "    Root location: ../data/validation\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "\n",
      "Test data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 494\n",
      "    Root location: ../data/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(180, 180)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dir = \"../data/train\"  # path to the train folder\n",
    "validation_dir = \"../data/validation\"  # path to the validation folder\n",
    "test_dir = \"../data/test\"  # path to test folder\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "\n",
    "validation_data = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "print(\n",
    "    f\"Train data:\\n{train_data}\\n\\nValidation data:\\n{validation_data}\\n\\nTest data:\\n{test_data}\"\n",
    ")\n",
    "\n",
    "data_loaders = {\n",
    "    \"train\": DataLoader(train_data, batch_size=4, shuffle=True, num_workers=2),\n",
    "    \"validation\": DataLoader(\n",
    "        validation_data, batch_size=4, shuffle=True, num_workers=2\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN3(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 22 * 22, 256)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.bn1(self.conv1(x))))\n",
    "        out = self.pool2(self.act2(self.bn2(self.conv2(out))))\n",
    "        out = self.pool3(self.act3(self.bn3(self.conv3(out))))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.act4(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = SimpleCNN3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / size\n",
    "    \n",
    "    return train_loss / len(dataloader), accuracy\n",
    "\n",
    "def validate(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    validation_loss, correct = 0, 0\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            validation_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / size\n",
    "    return validation_loss / len(dataloader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss funciton, optimizer and epochs\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rates = [1e-4, 1e-3, 5e-2, 1e-2, 5e-1]\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate: 0.0001\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(data_loaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m], model, loss_fn)\n\u001b[0;32m     15\u001b[0m     train_accuracies\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     15\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {\"lr\": [], \"train_acc\": [], \"validation_acc\": []}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with learning rate: {lr}\")\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    train_accuracies = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        train_loss, train_acc = train(data_loaders[\"train\"], model, loss_fn, optimizer)\n",
    "        val_loss, val_acc = validate(data_loaders[\"validation\"], model, loss_fn)\n",
    "\n",
    "        train_accuracies.append(train_acc)\n",
    "        validation_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Train Accuracy: {train_acc:.2f}%, Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    results[\"lr\"].append(lr)\n",
    "    results[\"train_acc\"].append(train_accuracies[-1])\n",
    "    results[\"validation_acc\"].append(validation_accuracies[-1])\n",
    "    print(f\"Last Train Accuracy for LR={lr}: {train_accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Train accuracy vs Learning rate\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mresults\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m], results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m], marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxscale(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Accuracy vs Learning Rate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Train accuracy vs Learning rate\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results[\"lr\"], results[\"train_acc\"], marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Train Accuracy vs Learning Rate\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Train Accuracy (%)\")\n",
    "\n",
    "# Validation accuracy vs Learning rate\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results[\"lr\"], results[\"validation_acc\"], marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Validation Accuracy vs Learning Rate\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Validation Accuracy (%)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
