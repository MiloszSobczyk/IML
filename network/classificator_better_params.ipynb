{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision import datasets\n",
    "from matplotlib.pyplot import imshow\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 3689\n",
      "    Root location: ./data/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "\n",
      "Validation data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 495\n",
      "    Root location: ./data/validation\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "\n",
      "Test data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 494\n",
      "    Root location: ./data/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([ \n",
    "    transforms.Resize(size=(180, 180)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dir= \"./data/train\" # path to the train folder\n",
    "validation_dir= \"./data/validation\" # path to the validation folder\n",
    "test_dir = \"./data/test\" # path to test folder\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=transform) \n",
    "\n",
    "validation_data = datasets.ImageFolder(root=validation_dir, \n",
    "                                 transform=transform)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir, \n",
    "                                 transform=transform)\n",
    "\n",
    "\n",
    "print(f\"Train data:\\n{train_data}\\n\\nValidation data:\\n{validation_data}\\n\\nTest data:\\n{test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1ecc3cab280>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1ece991f8e0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1ecc3ca9720>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_workers = 2\n",
    "\n",
    "train_set = DataLoader(dataset=train_data, \n",
    "                              batch_size=batch_size, # how many samples per batch?\n",
    "                              num_workers=num_workers, # how many subprocesses to use for data loading? (higher = more)\n",
    "                              shuffle=True) # shuffle the data?\n",
    "\n",
    "validation_set = DataLoader(dataset=validation_data, \n",
    "                             batch_size=batch_size, \n",
    "                             num_workers=num_workers, \n",
    "                             shuffle=True) # dont usually need to shuffle validation data\n",
    "\n",
    "test_set = DataLoader(dataset=test_data, \n",
    "                             batch_size=batch_size, \n",
    "                             num_workers=num_workers, \n",
    "                             shuffle=True) # dont usually need to shuffle testing data\n",
    "\n",
    "train_set,validation_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 11 * 11, 512)\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.bn1(self.conv1(x))))\n",
    "        out = self.pool2(self.act2(self.bn2(self.conv2(out))))\n",
    "        out = self.pool3(self.act3(self.bn3(self.conv3(out))))\n",
    "        out = self.pool4(self.act4(self.bn4(self.conv4(out))))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.act5(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "model = SimpleCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to add accuracies to these lists and I will use them outside of this function \n",
    "train_accuracies=[]\n",
    "validation_accuracies=[]\n",
    "\n",
    "# Function for training\n",
    "def train(dataloader, model, loss_fn, optimizer, epoch):\n",
    "    \n",
    "    size = len(dataloader.dataset) # total number of images inside of loader\n",
    "    num_batches = len(dataloader) # number of batches\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    train_loss, correct = 0, 0\n",
    "    \n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # move X and y to GPU for faster training\n",
    "        X, y = X.to(device), y.to(device) \n",
    "\n",
    "        # make prediction \n",
    "        pred = model(X)\n",
    "        # calculate loss \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward() # compute parameters gradients\n",
    "        optimizer.step() # update parameters\n",
    "        optimizer.zero_grad() #  reset the gradients of all parameters\n",
    "\n",
    "        # Update training loss\n",
    "        train_loss += loss.item() # item() method extracts the lossâ€™s value as a Python float\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'batch {batch} completed, train_loss={train_loss}')\n",
    "    \n",
    "    # loss and accuracy\n",
    "    train_loss = train_loss / num_batches\n",
    "    accuracy = 100 * correct / size\n",
    "    \n",
    "    # use this accuracy list for plotting accuracy with matplotlib\n",
    "    train_accuracies.append(accuracy)\n",
    "\n",
    "    # Print training accuracy and loss at the end of epoch\n",
    "    print(f\" Training Accuracy: {accuracy:.2f}%, Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "def validation(dataloader, model, loss_fn, t):\n",
    "    \n",
    "    size = len(dataloader.dataset) # total number of images inside of loader\n",
    "    num_batches = len(dataloader) # number of batches\n",
    "    \n",
    "    validation_loss, correct = 0, 0\n",
    "    \n",
    "    # sets the PyTorch model to evaluation mode, it will disable dropout layer\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad(): #  disable gradient calculation\n",
    "        for X, y in dataloader:\n",
    "            \n",
    "            # move X and y to GPU for faster training\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X) # make prediction\n",
    "            validation_loss += loss_fn(pred, y).item() \n",
    "            \n",
    "            # if prediction is correct add 1 to correct variable.\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    # loss and accuracy\n",
    "    validation_loss /= num_batches\n",
    "    accuracy = 100 * correct / size\n",
    "\n",
    "    validation_accuracies.append(accuracy)\n",
    "\n",
    "    # Print test accuracy and loss at the end of epoch\n",
    "    print(f\" Validation Accuracy: {accuracy:.2f}%, Validation Loss: {validation_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss funciton, optimizer and epochs\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "batch 0 completed, train_loss=0.6767285466194153\n",
      "batch 100 completed, train_loss=70.16140996292233\n",
      "batch 200 completed, train_loss=133.44081547483802\n",
      "batch 300 completed, train_loss=195.34774617478251\n",
      "batch 400 completed, train_loss=255.89916813001037\n",
      "batch 500 completed, train_loss=316.8931905888021\n",
      "batch 600 completed, train_loss=374.76616422459483\n",
      "batch 700 completed, train_loss=438.39289385452867\n",
      "batch 800 completed, train_loss=499.00533770397305\n",
      "batch 900 completed, train_loss=558.5208303891122\n",
      " Training Accuracy: 69.78%, Training Loss: 0.6188\n",
      " Validation Accuracy: 69.49%, Validation Loss: 0.5773\n",
      "----------------------------\n",
      "Epoch 2\n",
      "batch 0 completed, train_loss=0.6727873086929321\n",
      "batch 100 completed, train_loss=60.51677933335304\n",
      "batch 200 completed, train_loss=116.26248650252819\n",
      "batch 300 completed, train_loss=173.7169876843691\n",
      "batch 400 completed, train_loss=233.44198049604893\n",
      "batch 500 completed, train_loss=286.5778951495886\n",
      "batch 600 completed, train_loss=341.6136859804392\n",
      "batch 700 completed, train_loss=395.5438236668706\n",
      "batch 800 completed, train_loss=449.28585497289896\n",
      "batch 900 completed, train_loss=499.5249892473221\n",
      " Training Accuracy: 70.81%, Training Loss: 0.5538\n",
      " Validation Accuracy: 48.89%, Validation Loss: 0.8863\n",
      "----------------------------\n",
      "Epoch 3\n",
      "batch 0 completed, train_loss=0.5415477752685547\n",
      "batch 100 completed, train_loss=44.11590186879039\n",
      "batch 200 completed, train_loss=88.64702653605491\n",
      "batch 300 completed, train_loss=138.19808213878423\n",
      "batch 400 completed, train_loss=181.05943122226745\n",
      "batch 500 completed, train_loss=219.157200101763\n",
      "batch 600 completed, train_loss=256.5142717435956\n",
      "batch 700 completed, train_loss=292.24588299449533\n",
      "batch 800 completed, train_loss=324.66616764990613\n",
      "batch 900 completed, train_loss=357.91414328245446\n",
      " Training Accuracy: 82.35%, Training Loss: 0.3962\n",
      " Validation Accuracy: 62.22%, Validation Loss: 0.7695\n",
      "----------------------------\n",
      "Epoch 4\n",
      "batch 0 completed, train_loss=0.8247383832931519\n",
      "batch 100 completed, train_loss=29.27948695165105\n",
      "batch 200 completed, train_loss=57.287149294512346\n",
      "batch 300 completed, train_loss=83.2209349416662\n",
      "batch 400 completed, train_loss=105.97909394989256\n",
      "batch 500 completed, train_loss=137.13168162986403\n",
      "batch 600 completed, train_loss=164.81035477406112\n",
      "batch 700 completed, train_loss=193.3783715658792\n",
      "batch 800 completed, train_loss=219.24579342767538\n",
      "batch 900 completed, train_loss=242.03785648765916\n",
      " Training Accuracy: 88.43%, Training Loss: 0.2685\n",
      " Validation Accuracy: 93.33%, Validation Loss: 0.1666\n",
      "----------------------------\n",
      "Epoch 5\n",
      "batch 0 completed, train_loss=0.07367828488349915\n",
      "batch 100 completed, train_loss=17.668035336071625\n",
      "batch 200 completed, train_loss=36.35924816125771\n",
      "batch 300 completed, train_loss=55.6275252981286\n",
      "batch 400 completed, train_loss=75.00423377827974\n",
      "batch 500 completed, train_loss=99.29864551249193\n",
      "batch 600 completed, train_loss=121.02719330840046\n",
      "batch 700 completed, train_loss=142.41797712043626\n",
      "batch 800 completed, train_loss=161.16508165630512\n",
      "batch 900 completed, train_loss=173.86286996951094\n",
      " Training Accuracy: 92.63%, Training Loss: 0.1914\n",
      " Validation Accuracy: 89.29%, Validation Loss: 0.2355\n",
      "----------------------------\n",
      "Epoch 6\n",
      "batch 0 completed, train_loss=0.008192031644284725\n",
      "batch 100 completed, train_loss=12.776238347432809\n",
      "batch 200 completed, train_loss=30.43130168922653\n",
      "batch 300 completed, train_loss=43.71073031406013\n",
      "batch 400 completed, train_loss=57.184638067205015\n",
      "batch 500 completed, train_loss=72.81280466550288\n",
      "batch 600 completed, train_loss=90.28635391988882\n",
      "batch 700 completed, train_loss=104.90520248070698\n",
      "batch 800 completed, train_loss=113.67592325957958\n",
      "batch 900 completed, train_loss=127.00489798678609\n",
      " Training Accuracy: 94.50%, Training Loss: 0.1405\n",
      " Validation Accuracy: 95.76%, Validation Loss: 0.1124\n",
      "----------------------------\n",
      "Epoch 7\n",
      "batch 0 completed, train_loss=0.3393858075141907\n",
      "batch 100 completed, train_loss=9.521042207663413\n",
      "batch 200 completed, train_loss=19.707568480545888\n",
      "batch 300 completed, train_loss=29.650034245411007\n",
      "batch 400 completed, train_loss=42.06499271053326\n",
      "batch 500 completed, train_loss=46.81722213945977\n",
      "batch 600 completed, train_loss=53.28173707533824\n",
      "batch 700 completed, train_loss=62.35848281272047\n",
      "batch 800 completed, train_loss=70.8557452586465\n",
      "batch 900 completed, train_loss=82.51773340948262\n",
      " Training Accuracy: 96.42%, Training Loss: 0.0933\n",
      " Validation Accuracy: 97.58%, Validation Loss: 0.0891\n",
      "----------------------------\n",
      "Epoch 8\n",
      "batch 0 completed, train_loss=0.016054360195994377\n",
      "batch 100 completed, train_loss=4.696195629412614\n",
      "batch 200 completed, train_loss=10.8493557063332\n",
      "batch 300 completed, train_loss=17.42288959208861\n",
      "batch 400 completed, train_loss=24.4570805596461\n",
      "batch 500 completed, train_loss=30.935649475419154\n",
      "batch 600 completed, train_loss=40.670483625675445\n",
      "batch 700 completed, train_loss=48.09185479146396\n",
      "batch 800 completed, train_loss=57.00780128768935\n",
      "batch 900 completed, train_loss=60.57866931886214\n",
      " Training Accuracy: 97.21%, Training Loss: 0.0667\n",
      " Validation Accuracy: 97.37%, Validation Loss: 0.0838\n",
      "----------------------------\n",
      "Epoch 9\n",
      "batch 0 completed, train_loss=6.365025910781696e-05\n",
      "batch 100 completed, train_loss=4.282744494966664\n",
      "batch 200 completed, train_loss=19.375837252717417\n",
      "batch 300 completed, train_loss=21.7903708969655\n",
      "batch 400 completed, train_loss=24.986749361819705\n",
      "batch 500 completed, train_loss=33.98635321590211\n",
      "batch 600 completed, train_loss=42.15047639637817\n",
      "batch 700 completed, train_loss=48.751125423053395\n",
      "batch 800 completed, train_loss=51.76046028104281\n",
      "batch 900 completed, train_loss=54.00405620114108\n",
      " Training Accuracy: 98.13%, Training Loss: 0.0597\n",
      " Validation Accuracy: 92.12%, Validation Loss: 0.2588\n",
      "----------------------------\n",
      "Epoch 10\n",
      "batch 0 completed, train_loss=0.0011190705699846148\n",
      "batch 100 completed, train_loss=1.3570759453641017\n",
      "batch 200 completed, train_loss=1.8651818961674849\n",
      "batch 300 completed, train_loss=2.30819220921523\n",
      "batch 400 completed, train_loss=9.161470162701045\n",
      "batch 500 completed, train_loss=12.528103698760114\n",
      "batch 600 completed, train_loss=19.396355269158228\n",
      "batch 700 completed, train_loss=21.24310756291039\n",
      "batch 800 completed, train_loss=31.605229573992993\n",
      "batch 900 completed, train_loss=37.88902313481159\n",
      " Training Accuracy: 98.67%, Training Loss: 0.0424\n",
      " Validation Accuracy: 92.73%, Validation Loss: 0.2035\n",
      "----------------------------\n",
      "Epoch 11\n",
      "batch 0 completed, train_loss=0.0034500316251069307\n",
      "batch 100 completed, train_loss=1.3575289666882764\n",
      "batch 200 completed, train_loss=3.857685218349671\n",
      "batch 300 completed, train_loss=5.093376849770086\n",
      "batch 400 completed, train_loss=6.333617137432119\n",
      "batch 500 completed, train_loss=6.547949498793168\n",
      "batch 600 completed, train_loss=6.767567544298112\n",
      "batch 700 completed, train_loss=6.991246034115992\n",
      "batch 800 completed, train_loss=7.376813983434218\n",
      "batch 900 completed, train_loss=7.736915962890308\n",
      " Training Accuracy: 99.86%, Training Loss: 0.0084\n",
      " Validation Accuracy: 98.59%, Validation Loss: 0.0650\n",
      "----------------------------\n",
      "Epoch 12\n",
      "batch 0 completed, train_loss=0.003203944768756628\n",
      "batch 100 completed, train_loss=0.12657391037676824\n",
      "batch 200 completed, train_loss=0.26528872800793657\n",
      "batch 300 completed, train_loss=0.3350300752008746\n",
      "batch 400 completed, train_loss=0.4087774978857901\n",
      "batch 500 completed, train_loss=0.5630225123066737\n",
      "batch 600 completed, train_loss=1.7065541954079784\n",
      "batch 700 completed, train_loss=3.239980044866501\n",
      "batch 800 completed, train_loss=3.5545018306598966\n",
      "batch 900 completed, train_loss=5.016655824852586\n",
      " Training Accuracy: 99.73%, Training Loss: 0.0067\n",
      " Validation Accuracy: 97.78%, Validation Loss: 0.0815\n",
      "----------------------------\n",
      "Epoch 13\n",
      "batch 0 completed, train_loss=2.9205340979387984e-05\n",
      "batch 100 completed, train_loss=3.5733286683410466\n",
      "batch 200 completed, train_loss=8.257520793483591\n",
      "batch 300 completed, train_loss=10.513575999949806\n",
      "batch 400 completed, train_loss=11.026324055954364\n",
      "batch 500 completed, train_loss=12.988060296642372\n",
      "batch 600 completed, train_loss=24.15750245139148\n",
      "batch 700 completed, train_loss=25.961678619327383\n",
      "batch 800 completed, train_loss=28.766450894323864\n",
      "batch 900 completed, train_loss=33.928686439061586\n",
      " Training Accuracy: 98.83%, Training Loss: 0.0376\n",
      " Validation Accuracy: 96.16%, Validation Loss: 0.1045\n",
      "----------------------------\n",
      "Epoch 14\n",
      "batch 0 completed, train_loss=0.05875309556722641\n",
      "batch 100 completed, train_loss=1.557212034801175\n",
      "batch 200 completed, train_loss=2.4733062322930337\n",
      "batch 300 completed, train_loss=2.887760541993835\n",
      "batch 400 completed, train_loss=3.0607331038214323\n",
      "batch 500 completed, train_loss=4.045260900379125\n",
      "batch 600 completed, train_loss=4.21656582909171\n",
      "batch 700 completed, train_loss=4.299254401111295\n",
      "batch 800 completed, train_loss=6.431484091757172\n",
      "batch 900 completed, train_loss=6.817769567159305\n",
      " Training Accuracy: 99.76%, Training Loss: 0.0079\n",
      " Validation Accuracy: 98.79%, Validation Loss: 0.0480\n",
      "----------------------------\n",
      "Epoch 15\n",
      "batch 0 completed, train_loss=3.1888296234683366e-06\n",
      "batch 100 completed, train_loss=0.05686131270925188\n",
      "batch 200 completed, train_loss=0.13870357842693082\n",
      "batch 300 completed, train_loss=0.3162522180768583\n",
      "batch 400 completed, train_loss=0.4365812195533074\n",
      "batch 500 completed, train_loss=0.5606568543471084\n",
      "batch 600 completed, train_loss=0.8040977349533271\n",
      "batch 700 completed, train_loss=0.8959948833304434\n",
      "batch 800 completed, train_loss=0.9866914577109291\n",
      "batch 900 completed, train_loss=1.0075986450592822\n",
      " Training Accuracy: 100.00%, Training Loss: 0.0011\n",
      " Validation Accuracy: 98.59%, Validation Loss: 0.0524\n",
      "----------------------------\n",
      "Epoch 16\n",
      "batch 0 completed, train_loss=1.460311068512965e-06\n",
      "batch 100 completed, train_loss=0.04217437242890654\n",
      "batch 200 completed, train_loss=0.07189112768688766\n",
      "batch 300 completed, train_loss=0.10076550606706292\n",
      "batch 400 completed, train_loss=0.14942542738405074\n",
      "batch 500 completed, train_loss=0.17388081440177316\n",
      "batch 600 completed, train_loss=0.2000665802346422\n",
      "batch 700 completed, train_loss=0.25723209272611847\n",
      "batch 800 completed, train_loss=0.28473053702238893\n",
      "batch 900 completed, train_loss=0.3626826020391327\n",
      " Training Accuracy: 100.00%, Training Loss: 0.0004\n",
      " Validation Accuracy: 98.79%, Validation Loss: 0.0481\n",
      "----------------------------\n",
      "Epoch 17\n",
      "batch 0 completed, train_loss=3.8146863516885787e-06\n",
      "batch 100 completed, train_loss=0.046368395310189214\n",
      "batch 200 completed, train_loss=0.08735525582218706\n",
      "batch 300 completed, train_loss=0.12737040669600042\n",
      "batch 400 completed, train_loss=0.14253813159131745\n",
      "batch 500 completed, train_loss=0.16974516218307834\n",
      "batch 600 completed, train_loss=0.19796989835893086\n",
      "batch 700 completed, train_loss=0.21765807537189197\n",
      "batch 800 completed, train_loss=0.23529657678100335\n",
      "batch 900 completed, train_loss=0.2516093345656074\n",
      " Training Accuracy: 100.00%, Training Loss: 0.0003\n",
      " Validation Accuracy: 98.59%, Validation Loss: 0.0512\n",
      "----------------------------\n",
      "Epoch 18\n",
      "batch 0 completed, train_loss=7.510132491006516e-06\n",
      "batch 100 completed, train_loss=0.009596724048419603\n",
      "batch 200 completed, train_loss=0.02498835598757232\n",
      "batch 300 completed, train_loss=0.042641364013778826\n",
      "batch 400 completed, train_loss=0.06955537871808026\n",
      "batch 500 completed, train_loss=0.08696076782506701\n",
      "batch 600 completed, train_loss=0.09868303943671819\n",
      "batch 700 completed, train_loss=0.11210805948530833\n",
      "batch 800 completed, train_loss=0.13385347550538462\n",
      "batch 900 completed, train_loss=0.15271702848236224\n",
      " Training Accuracy: 100.00%, Training Loss: 0.0002\n",
      " Validation Accuracy: 98.59%, Validation Loss: 0.0564\n",
      "----------------------------\n",
      "Epoch 19\n",
      "batch 0 completed, train_loss=0.00019473138672765344\n",
      "batch 100 completed, train_loss=0.018346667827918495\n",
      "batch 200 completed, train_loss=0.03282015815225492\n",
      "batch 300 completed, train_loss=0.13491609931387494\n",
      "batch 400 completed, train_loss=0.16865306761934562\n",
      "batch 500 completed, train_loss=0.1875956517822832\n",
      "batch 600 completed, train_loss=0.19960846742015903\n",
      "batch 700 completed, train_loss=0.2116919141456357\n",
      "batch 800 completed, train_loss=0.2258936836464951\n",
      "batch 900 completed, train_loss=0.2324781550962367\n",
      " Training Accuracy: 100.00%, Training Loss: 0.0003\n",
      " Validation Accuracy: 98.79%, Validation Loss: 0.0580\n",
      "----------------------------\n",
      "Epoch 20\n",
      "batch 0 completed, train_loss=0.00032139019458554685\n",
      "batch 100 completed, train_loss=0.01365285955897022\n",
      "batch 200 completed, train_loss=0.0226251186392048\n",
      "batch 300 completed, train_loss=0.0375097898760508\n",
      "batch 400 completed, train_loss=0.08140649181616588\n",
      "batch 500 completed, train_loss=0.09564869975611501\n",
      "batch 600 completed, train_loss=0.10494178199654236\n",
      "batch 700 completed, train_loss=0.12328331458052944\n",
      "batch 800 completed, train_loss=0.12963779726631586\n",
      "batch 900 completed, train_loss=0.14874328605811016\n",
      " Training Accuracy: 100.00%, Training Loss: 0.0002\n",
      " Validation Accuracy: 98.59%, Validation Loss: 0.0610\n",
      "----------------------------\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    \n",
    "    train(train_set, model, loss_fn, optimizer, t)\n",
    "    validation(validation_set, model, loss_fn, t)\n",
    "    print(\"----------------------------\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"accept\", \"refuse\"]\n",
    "\n",
    "dataiter = iter(test_set)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c716275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modified validation function to compute F1 score\n",
    "def validation(dataloader, model, loss_fn, t):\n",
    "    size = len(dataloader.dataset) # total number of images in loader\n",
    "    num_batches = len(dataloader) # number of batches\n",
    "    validation_loss, correct = 0, 0\n",
    "    \n",
    "    # Store true and predicted labels for F1 score calculation\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    model.eval() # Set model to evaluation mode (disables dropout)\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X) # Make prediction\n",
    "            validation_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            # Collect predictions and labels for F1\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    # Loss and accuracy\n",
    "    validation_loss /= num_batches\n",
    "    accuracy = 100 * correct / size\n",
    "    validation_accuracies.append(accuracy)\n",
    "    \n",
    "    # Calculate macro-averaged F1 score\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # Print validation metrics\n",
    "    print(f\" Validation Accuracy: {accuracy:.2f}%, Validation Loss: {validation_loss:.4f}, F1 Score (Macro): {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation on validation set:\n",
      " Validation Accuracy: 98.79%, Validation Loss: 0.0492, F1 Score (Macro): 0.9855\n"
     ]
    }
   ],
   "source": [
    "print(\"Final evaluation on validation set:\")\n",
    "validation(test_set, model, loss_fn, t=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"./nets/\"\n",
    "\n",
    "torch.save(model.state_dict(), \"./nets/skynet5.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
