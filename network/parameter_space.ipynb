{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.23.3)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision import datasets\n",
    "from matplotlib.pyplot import imshow\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 3689\n",
      "    Root location: ./data/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "\n",
      "Validation data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 495\n",
      "    Root location: ./data/validation\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "\n",
      "Test data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 494\n",
      "    Root location: ./data/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([ \n",
    "    transforms.Resize(size=(180, 180)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dir= \"./data/train\" # path to the train folder\n",
    "validation_dir= \"./data/validation\" # path to the validation folder\n",
    "test_dir = \"./data/test\" # path to test folder\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=transform) \n",
    "\n",
    "validation_data = datasets.ImageFolder(root=validation_dir, \n",
    "                                 transform=transform)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir, \n",
    "                                 transform=transform)\n",
    "\n",
    "\n",
    "print(f\"Train data:\\n{train_data}\\n\\nValidation data:\\n{validation_data}\\n\\nTest data:\\n{test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x242bcff6f80>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x242bcff5db0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x242bcff7a30>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_workers = 2\n",
    "\n",
    "train_set = DataLoader(dataset=train_data, \n",
    "                              batch_size=batch_size, # how many samples per batch?\n",
    "                              num_workers=num_workers, # how many subprocesses to use for data loading? (higher = more)\n",
    "                              shuffle=True) # shuffle the data?\n",
    "\n",
    "validation_set = DataLoader(dataset=validation_data, \n",
    "                             batch_size=batch_size, \n",
    "                             num_workers=num_workers, \n",
    "                             shuffle=True) # dont usually need to shuffle validation data\n",
    "\n",
    "test_set = DataLoader(dataset=test_data, \n",
    "                             batch_size=batch_size, \n",
    "                             num_workers=num_workers, \n",
    "                             shuffle=True) # dont usually need to shuffle testing data\n",
    "\n",
    "train_set,validation_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modelv4 import SimpleCNN4\n",
    "model = SimpleCNN4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to add accuracies to these lists and I will use them outside of this function \n",
    "train_accuracies=[]\n",
    "validation_accuracies=[]\n",
    "\n",
    "# Function for training\n",
    "def train(dataloader, model, loss_fn, optimizer, epoch):\n",
    "    \n",
    "    size = len(dataloader.dataset) # total number of images inside of loader\n",
    "    num_batches = len(dataloader) # number of batches\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    train_loss, correct = 0, 0\n",
    "    \n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # move X and y to GPU for faster training\n",
    "        X, y = X.to(device), y.to(device) \n",
    "\n",
    "        # make prediction \n",
    "        pred = model(X)\n",
    "        # calculate loss \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward() # compute parameters gradients\n",
    "        optimizer.step() # update parameters\n",
    "        optimizer.zero_grad() #  reset the gradients of all parameters\n",
    "\n",
    "        # Update training loss\n",
    "        train_loss += loss.item() # item() method extracts the loss’s value as a Python float\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'batch {batch} completed, train_loss={train_loss}')\n",
    "    \n",
    "    # loss and accuracy\n",
    "    train_loss = train_loss / num_batches\n",
    "    accuracy = 100 * correct / size\n",
    "    \n",
    "    # use this accuracy list for plotting accuracy with matplotlib\n",
    "    train_accuracies.append(accuracy)\n",
    "\n",
    "    # Print training accuracy and loss at the end of epoch\n",
    "    print(f\" Training Accuracy: {accuracy:.2f}%, Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "def validation(dataloader, model, loss_fn, t):\n",
    "    \n",
    "    size = len(dataloader.dataset) # total number of images inside of loader\n",
    "    num_batches = len(dataloader) # number of batches\n",
    "    \n",
    "    validation_loss, correct = 0, 0\n",
    "    \n",
    "    # sets the PyTorch model to evaluation mode, it will disable dropout layer\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad(): #  disable gradient calculation\n",
    "        for X, y in dataloader:\n",
    "            \n",
    "            # move X and y to GPU for faster training\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X) # make prediction\n",
    "            validation_loss += loss_fn(pred, y).item() \n",
    "            \n",
    "            # if prediction is correct add 1 to correct variable.\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    # loss and accuracy\n",
    "    validation_loss /= num_batches\n",
    "    accuracy = 100 * correct / size\n",
    "\n",
    "    validation_accuracies.append(accuracy)\n",
    "\n",
    "    # Print test accuracy and loss at the end of epoch\n",
    "    print(f\" Validation Accuracy: {accuracy:.2f}%, Validation Loss: {validation_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_learning_parameters = False\n",
    "\n",
    "def log_weights(model, epoch):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            np.save(f\"'weights_epoch_{epoch}_{name}.npy\", param.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers 2-norm diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l2_diff(param, init_param):\n",
    "    \"\"\"Compute L2 norm of the difference between current param and initial param.\"\"\"\n",
    "    return torch.norm(param - init_param)\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "model = SimpleCNN4(num_classes=2).to(device)\n",
    "\n",
    "# Zachowujemy stan początkowy wszystkich wag (tzw. snapshot)\n",
    "init_params = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# Słownik na listy z odległościami L2 (względem wag początkowych) dla każdej nazwy parametru\n",
    "l2_diffs = {name: [] for name in model.state_dict().keys()}\n",
    "\n",
    "# Listy na dokładność dla zbioru treningowego i testowego\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n=== EPOCH {epoch+1} / {epochs} ===\")\n",
    "\n",
    "    # ------ Training loop ------\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X, y in train_set:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        correct += (pred.argmax(dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    train_accuracy = 100.0 * correct / total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_set:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_correct += (pred.argmax(dim=1) == y).sum().item()\n",
    "            test_total += y.size(0)\n",
    "    test_accuracy = 100.0 * test_correct / test_total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    print(\n",
    "        f\"Train Accuracy: {train_accuracy:.2f}% | Test Accuracy: {test_accuracy:.2f}%\"\n",
    "    )\n",
    "    current_params = model.state_dict()\n",
    "    for name in current_params.keys():\n",
    "        # Skip any non-float parameters (like num_batches_tracked which is LongTensor)\n",
    "        if current_params[name].dtype in (torch.float32, torch.float64):\n",
    "            diff = compute_l2_diff(current_params[name], init_params[name])\n",
    "            l2_diffs[name].append(diff.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "epochs_range = range(1, epochs + 1)\n",
    "\n",
    "for layer_name in [\"conv1.weight\", \"conv2.weight\", \"conv3.weight\", \"conv4.weight\"]:\n",
    "    short_label = layer_name.replace(\n",
    "        \".weight\", \"\"\n",
    "    )  # np. \"conv1\" zamiast \"conv1.weight\"\n",
    "    plt.plot(epochs_range, l2_diffs[layer_name], label=short_label)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"L2 distance to initial\")\n",
    "plt.title(\"Layer-wise L2 Distance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll assume you have something like this:\n",
    "# model = SimpleCNN4(num_classes=2).to(device)\n",
    "# train_set, test_set = your DataLoaders\n",
    "\n",
    "\n",
    "def test_accuracy(model, test_loader, device):\n",
    "    \"\"\"Measure test accuracy of the model on test_set.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            correct += (pred.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "\n",
    "def run_experiment(layer_name, mode, device, epochs=20, reinit_interval=1, lr=1e-3):\n",
    "    \"\"\"\n",
    "    layer_name: e.g. 'conv1', 'conv2', ...\n",
    "    mode: 'randomize' or 'reinit'\n",
    "    epochs: total epochs to train\n",
    "    reinit_interval: how often to 'mess up' the layer (e.g. every epoch)\n",
    "    lr: learning rate\n",
    "    \"\"\"\n",
    "    model = SimpleCNN4()\n",
    "    init_params = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    accuracy_log = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ---- Training loop ----\n",
    "        model.train()\n",
    "        for X, y in train_set:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # ---- Compute test accuracy ----\n",
    "        acc = test_accuracy(model, test_set, device)\n",
    "        accuracy_log.append(acc)\n",
    "\n",
    "        # ---- \"Mess up\" the chosen layer if needed ----\n",
    "        # We'll do it every \"reinit_interval\" epochs\n",
    "        if (epoch + 1) % reinit_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                if mode == \"randomize\":\n",
    "                    # Re-randomize the weights of the chosen layer\n",
    "                    # Example for conv1: model.conv1.weight, model.conv1.bias\n",
    "                    layer_weight = getattr(model, layer_name).weight\n",
    "                    layer_bias = getattr(model, layer_name).bias\n",
    "                    nn.init.xavier_uniform_(layer_weight)\n",
    "                    nn.init.zeros_(layer_bias)\n",
    "                elif mode == \"reinit\":\n",
    "                    # Re-initialize the chosen layer to original weights\n",
    "                    # We load from init_params dictionary\n",
    "                    layer_weight = getattr(model, layer_name).weight\n",
    "                    layer_bias = getattr(model, layer_name).bias\n",
    "                    layer_weight.copy_(init_params[f\"{layer_name}.weight\"])\n",
    "                    layer_bias.copy_(init_params[f\"{layer_name}.bias\"])\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown mode! Choose 'randomize' or 'reinit'.\")\n",
    "\n",
    "    return accuracy_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll define the layers we want to test\n",
    "layers_to_test = [\"conv1\", \"conv2\", \"conv3\", \"conv4\"]  # Example from SimpleCNN4\n",
    "epochs = 10  # or 100 if you want a longer experiment\n",
    "\n",
    "# We'll create dictionaries to store the results\n",
    "re_randomize_results = {}\n",
    "re_init_results = {}\n",
    "\n",
    "for layer in layers_to_test:\n",
    "    print(f\"Running re-randomize experiment for {layer}...\")\n",
    "    re_randomize_results[layer] = run_experiment(\n",
    "        layer, mode=\"randomize\", device=device, epochs=epochs, reinit_interval=1\n",
    "    )\n",
    "\n",
    "    print(f\"Running re-init experiment for {layer}...\")\n",
    "    re_init_results[layer] = run_experiment(\n",
    "        layer, mode=\"reinit\", device=device, epochs=epochs, reinit_interval=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll define the layers we want to test\n",
    "layers_to_test = [\"conv1\", \"conv2\", \"conv3\", \"conv4\"]  # Example from SimpleCNN4\n",
    "epochs = 10  # or 100 if you want a longer experiment\n",
    "\n",
    "# We'll create dictionaries to store the results\n",
    "re_randomize_results = {}\n",
    "re_init_results = {}\n",
    "\n",
    "for layer in layers_to_test:\n",
    "    print(f\"Running re-randomize experiment for {layer}...\")\n",
    "    re_randomize_results[layer] = run_experiment(\n",
    "        layer, mode=\"randomize\", device=device, epochs=epochs, reinit_interval=1\n",
    "    )\n",
    "\n",
    "    print(f\"Running re-init experiment for {layer}...\")\n",
    "    re_init_results[layer] = run_experiment(\n",
    "        layer, mode=\"reinit\", device=device, epochs=epochs, reinit_interval=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can plot the results:\n",
    "# 1) Re-randomization robustness\n",
    "plt.figure(figsize=(8, 6))\n",
    "for layer in layers_to_test:\n",
    "    plt.plot(range(1, epochs + 1), re_randomize_results[layer], label=f\"{layer}\")\n",
    "plt.xlabel(\"num. training epoch\")\n",
    "plt.ylabel(\"test accuracy (%)\")\n",
    "plt.title(\"Re-randomization robustness\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2) Re-initialization robustness\n",
    "plt.figure(figsize=(8, 6))\n",
    "for layer in layers_to_test:\n",
    "    plt.plot(range(1, epochs + 1), re_init_results[layer], label=f\"{layer}\")\n",
    "plt.xlabel(\"num. training epoch\")\n",
    "plt.ylabel(\"test accuracy (%)\")\n",
    "plt.title(\"Re-initialization robustness\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
